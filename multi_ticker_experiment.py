# ===============================================================
#  multi_ticker_experiment.py ‚Äî –ú–∞—Å—Å–æ–≤–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ —Ç–∏–∫–µ—Ä–∞–º
# ---------------------------------------------------------------
#  –ó–∞–ø—É—Å–∫–∞–µ—Ç llm_finance_predictor.py –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤:
#   ‚Ä¢ –ü–µ—Ä–µ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã —Å –∫–æ—Ç–∏—Ä–æ–≤–∫–∞–º–∏ –≤ /Data
#   ‚Ä¢ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–∏–∫–µ—Ä—É –≤ /results
#   ‚Ä¢ –í—ã—á–∏—Å–ª—è–µ—Ç —É—Å—Ä–µ–¥–Ω—ë–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ä—ã–Ω–∫—É
#
#  –£–¥–æ–±–Ω–æ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ –±—É–º–∞–≥.
#
#  –ê–≤—Ç–æ—Ä: –ú–∏—Ö–∞–∏–ª –®–∞—Ä–¥–∏–Ω
#  –û–Ω–ª–∞–π–Ω-–≤–∏–∑–∏—Ç–∫–∞: https://shardin.name/?utm_source=python
# 
#  –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: https://github.com/empenoso/llm-stock-market-predictor
# ===============================================================

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List
import json
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import torch
import shutil

from llm_finance_predictor import LLMFinancialPredictor, WalkForwardValidator

class MultiTickerExperiment:
    """–ü—Ä–æ–≤–æ–¥–∏—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ —Ç–∏–∫–µ—Ä–æ–≤"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.data_dir = Path(config['data_dir'])
        self.results_dir = Path(config['results_dir'])
        self.results_dir.mkdir(exist_ok=True, parents=True)
        
        self.predictor = LLMFinancialPredictor(model_name=config['model_name'])
        self.validator = WalkForwardValidator(
            train_size=config['train_size'], 
            test_size=config['test_size'], 
            step_size=config['step_size']
        )
    
    def load_all_data(self) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤ –æ–¥–∏–Ω DataFrame –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.
        """
        ticker_files = sorted(self.data_dir.glob("*.txt"))
        if self.config['max_tickers']:
            ticker_files = ticker_files[:self.config['max_tickers']]
        
        all_dfs = []
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ {len(ticker_files)} —Ñ–∞–π–ª–æ–≤...")
        for file in tqdm(ticker_files, desc="–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤"):
            try:
                df = self.predictor.load_data(file)
                df['ticker'] = file.stem.split('_')[0] 
                all_dfs.append(df)
            except Exception as e:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {file}: {e}")
        
        if not all_dfs:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
            
        return pd.concat(all_dfs, ignore_index=True)

    def run(self):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: –∑–∞–≥—Ä—É–∑–∫–∞, –ø–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞, –æ–±—É—á–µ–Ω–∏–µ –ø–æ —Ç–∏–∫–µ—Ä–∞–º, –æ—Ü–µ–Ω–∫–∞.
        """
        print("="*80)
        print("–ù–ê–ß–ê–õ–û –ú–ê–°–®–¢–ê–ë–ù–û–ì–û –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê")
        print(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {self.config}")
        print("="*80)

        # 1. –ü–∞–∫–µ—Ç–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
        try:
            combined_df = self.load_all_data()
        except ValueError as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            return

        # 2. –ü–∞–∫–µ—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - –ï–î–ò–ù–°–¢–í–ï–ù–ù–´–ô –í–´–ó–û–í –ù–ê –í–°–ï –î–ê–ù–ù–´–ï
        print("\nüîß –ü–∞–∫–µ—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è.")
        features_df = self.predictor.feature_extractor.process_dataframe(combined_df)
        print(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏: {len(features_df)}")
        
        # 3. –ò—Ç–µ—Ä–∞—Ü–∏—è –ø–æ —Ç–∏–∫–µ—Ä–∞–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        all_results = []
        tickers = features_df['ticker'].unique()
        
        for ticker_name in tqdm(tickers, desc="–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ —Ç–∏–∫–µ—Ä–∞–º"):
            ticker_features = features_df[features_df['ticker'] == ticker_name].copy()
            
            if len(ticker_features) < (self.validator.train_size + self.validator.test_size):
                tqdm.write(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ {ticker_name}: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(ticker_features)} —Å—Ç—Ä–æ–∫)")
                all_results.append({'ticker': ticker_name, 'status': 'skipped_insufficient_data'})
                continue

            splits = self.validator.split(ticker_features)
            if not splits:
                all_results.append({'ticker': ticker_name, 'status': 'skipped_no_splits'})
                continue
            
            if self.config['max_folds']:
                splits = splits[:self.config['max_folds']]

            fold_results = []
            for i, (train_df, test_df) in enumerate(splits):
                train_texts, train_labels = self.predictor.prepare_dataset(train_df)
                test_texts, test_labels = self.predictor.prepare_dataset(test_df)

                if np.mean(train_labels) < 0.3 or np.mean(train_labels) > 0.7:
                    continue

                self.predictor.train(train_texts, train_labels, epochs=self.config['epochs'], batch_size=self.config['batch_size'])
                metrics = self.predictor.evaluate(test_texts, test_labels)
                fold_results.append(metrics)
                
                # –û—á–∏—Å—Ç–∫–∞ –º–µ—Å—Ç–∞ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
                shutil.rmtree('./results/training_output', ignore_errors=True)
                shutil.rmtree('./logs', ignore_errors=True)


            if not fold_results:
                all_results.append({'ticker': ticker_name, 'status': 'failed_no_valid_folds'})
                continue

            avg_metrics = {k: np.mean([r[k] for r in fold_results]) for k in fold_results[0]}
            std_metrics = {f"{k}_std": np.std([r[k] for r in fold_results]) for k in ['accuracy', 'f1', 'auc']}
            
            result = {'ticker': ticker_name, 'status': 'success', 'n_folds': len(fold_results), **avg_metrics, **std_metrics}
            all_results.append(result)
            
            tqdm.write(f"‚úÖ {ticker_name}: AUC = {result['auc']:.4f} ¬± {result['auc_std']:.4f}")

        # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∞–Ω–∞–ª–∏–∑
        if all_results:
            self.save_results(all_results)
            self.analyze_results(all_results)
        
    def save_results(self, results: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filepath = self.results_dir / f"experiment_results_{timestamp}.json"
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {filepath}")
    
    def analyze_results(self, results: List[Dict]):
        """–ü—Ä–æ–≤–æ–¥–∏—Ç –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ —Å–æ–∑–¥–∞–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏"""
        print("\n" + "="*80 + "\n–ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\n" + "="*80)
        
        successful = [r for r in results if r.get('status') == 'success']
        if not successful:
            print("‚ùå –ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
            return

        metrics_df = pd.DataFrame(successful).round(4)
        print(f"\nüìà –û–±—â–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–ø–æ {len(successful)} —Ç–∏–∫–µ—Ä–∞–º):")
        print(metrics_df[['ticker', 'auc', 'accuracy', 'f1']].sort_values('auc', ascending=False).to_string(index=False))
        
        print("\n" + "-"*60)
        for metric in ['accuracy', 'precision', 'recall', 'f1', 'auc']:
            values = metrics_df[metric]
            print(f"{metric.upper():>12s}: Mean={values.mean():.4f}, Std={values.std():.4f}, Median={values.median():.4f}")
        
        self.create_visualizations(metrics_df)

    def create_visualizations(self, metrics_df: pd.DataFrame):
        """–°–æ–∑–¥–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏"""
        plt.style.use('seaborn-v0_8-whitegrid')
        fig, axes = plt.subplots(1, 2, figsize=(20, 8))
        
        sns.histplot(metrics_df['auc'], kde=True, ax=axes[0], bins=15, color='royalblue')
        axes[0].axvline(0.5, color='r', linestyle='--', label='Random Guess (0.5)')
        axes[0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ AUC –ø–æ –≤—Å–µ–º —Ç–∏–∫–µ—Ä–∞–º')
        axes[0].set_xlabel('AUC Score')
        axes[0].legend()
        
        top_performers = metrics_df.nlargest(15, 'auc').sort_values('auc', ascending=True)
        axes[1].barh(top_performers['ticker'], top_performers['auc'], color='skyblue')
        axes[1].axvline(0.5, color='r', linestyle='--')
        axes[1].set_title('–¢–æ–ø-15 —Ç–∏–∫–µ—Ä–æ–≤ –ø–æ —Å—Ä–µ–¥–Ω–µ–π AUC')
        axes[1].set_xlabel('AUC Score')
        
        plt.tight_layout()
        filepath = self.results_dir / f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        plt.savefig(filepath, dpi=300)
        print(f"\nüìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {filepath}")
        plt.close()

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞"""
    ## –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: –í—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤—ã–Ω–µ—Å–µ–Ω–∞ –≤ –æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    CONFIG = {
        'data_dir': 'Data/Tinkoff',
        'results_dir': 'results',
        'model_name': 'distilbert-base-uncased',
        'max_tickers': None, # None –¥–ª—è –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤, 20 –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
        'max_folds': 3,
        'epochs': 2,
        'batch_size': 32,
        'train_size': 252, # 1 –≥–æ–¥
        'test_size': 21,   # 1 –º–µ—Å—è—Ü
        'step_size': 21
    }
    
    experiment = MultiTickerExperiment(config=CONFIG)
    experiment.run()

if __name__ == "__main__":
    if torch.cuda.is_available():
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω GPU: {torch.cuda.get_device_name(0)}")
    else:
        print("‚ö†Ô∏è GPU –Ω–µ –Ω–∞–π–¥–µ–Ω. –†–∞—Å—á–µ—Ç—ã –±—É–¥—É—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –Ω–∞ CPU.")
    main()